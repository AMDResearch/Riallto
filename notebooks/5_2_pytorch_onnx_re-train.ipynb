{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and ONNX Flow for Training\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Learn how to re-training a model using PyTorch\n",
    "\n",
    "* Learn how to export a trained model to ONNX\n",
    "\n",
    "* Learn how to quantize an ONNX model to run inference on the NPU\n",
    "\n",
    "## References\n",
    "\n",
    "**[Ryzen AI Software Platform](https://ryzenai.docs.amd.com/en/latest/getstartex.html)**\n",
    "\n",
    "**[Vitis AI Execution Provider](https://onnxruntime.ai/docs/execution-providers/Vitis-AI-ExecutionProvider.html)**\n",
    "\n",
    "**[CIFAR10](https://github.com/EN10/CIFAR)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\"> \n",
    "    \n",
    "Running this re-training notebook will generate model files that will overwrite the existing trained quantized file in the `onnx` folder.\n",
    "\n",
    "Please make sure you rename any existing model files in the `onnx` folder to save them.\n",
    "\n",
    "The names of the model files that will be written are the following:\n",
    "\n",
    "1. The trained ResNet-50 model on the CIFAR-10 dataset is: `onnx\\resnet_trained_for_cifar10.pt`.\n",
    "2. The trained ResNet-50 model on the CIFAR-10 dataset in ONNX format is: `onnx\\resnet_trained_for_cifar10.onnx`.\n",
    "3. The trained quantized ResNet-50 model on the CIFAR-10 dataset in ONNX format is: `onnx/resnet.qdq.U8S8.onnx`\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import vai_q_onnx\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us retrain the [ResNet-50 model](https://arxiv.org/pdf/1512.03385.pdf) from PyTorch Hub using the CIFAR-10 dataset.\n",
    "\n",
    "The CIFAR-10 dataset is used to retrain the default model using the [transfer learning technique](https://www.youtube.com/watch?v=BqqfQnyjmgg&list=PLo2EIpI_JMQtNtKNFFSMNIZwspj8H7-sQ&index=3).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "Make sure that the CIFAR-10 dataset is downloaded. For steps refer to the previous notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model for re-training using transfer learning\n",
    "\n",
    "The pre-trained ResNet-50 model trained on 1,000 class ImageNet dataset by default has fully connected (FC) layer of output size 1,000. This means that it produces a 1,000-dimensional vector, where each dimension corresponds to a class in the ImageNet dataset.\n",
    "\n",
    "We use transfer learning to select a set of pre-trained weights for the model and then customize the model's classifier by replacing its FC layers. The modification includes adding two linear layers, one with 2,048 input features and 64 output features, followed by a ReLU activation function, and another linear layer with 64 input features and 10 output features. This adaptation transforms the ResNet-50 model into a classifier suitable for a specific task with 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 1 (see end of notebook)\n",
    "\n",
    "def load_resnet_model():\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    resnet = resnet50(weights=weights)\n",
    "    resnet.fc = torch.nn.Sequential(torch.nn.Linear(2048, 64), torch.nn.ReLU(inplace=True), torch.nn.Linear(64, 10))\n",
    "    return resnet\n",
    "\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model re-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CIFAR-10 dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global models_dir, data_dir\n",
    "models_dir = \".\\\\onnx\"\n",
    "data_dir= \".\\\\onnx\\\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process runs over 500 images with a `batch_size` of 100, i.e., over the total 50,000 images in the train set.\n",
    "\n",
    "The training process takes approximately 10 minutes to complete each epoch. Number of epochs can be varied to optimize the accuracy of the model.\n",
    "\n",
    "At the end of this process, we will save the trained model as an ONNX model and then we will also quantize this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# License 1 (see end of notebook)\n",
    "\n",
    "def prepare_model(num_epochs=0):\n",
    "    # Seed everything to 0\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Hyper-parameters\n",
    "    num_epochs = num_epochs\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Image preprocessing modules\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]\n",
    "    )\n",
    "\n",
    "    # CIFAR-10 dataset\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=False)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model = load_resnet_model().to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    curr_lr = learning_rate\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    \"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(\n",
    "                        epoch + 1, num_epochs, i + 1, total_step, loss.item()\n",
    "                    )\n",
    "                )\n",
    "        # Decay learning rate\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            curr_lr /= 3\n",
    "            update_lr(optimizer, curr_lr)\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    if num_epochs:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            print(\"Accuracy of the model on the test images: {} %\".format(accuracy))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/500] Loss: 1.0127\n",
      "Epoch [1/1], Step [200/500] Loss: 0.9769\n",
      "Epoch [1/1], Step [300/500] Loss: 0.8371\n",
      "Epoch [1/1], Step [400/500] Loss: 0.4864\n",
      "Epoch [1/1], Step [500/500] Loss: 0.7878\n",
      "Accuracy of the model on the test images: 77.61 %\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model = prepare_model(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained PyTorch model by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model_path = f\"{models_dir}/resnet_trained_for_cifar10.pt\"\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the training process, observe the following output:   \n",
    "\n",
    "* The trained ResNet-50 model on the CIFAR-10 dataset is saved at the following location: `onnx/resnet_trained_for_cifar10.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert Model to ONNX Format\n",
    "\n",
    "Run the following cell to save the trained model as an ONNX model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_onnx_model(model):\n",
    "    dummy_inputs = torch.randn(1, 3, 32, 32)\n",
    "    input_names = ['input']\n",
    "    output_names = ['output']\n",
    "    dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    onnx_model_path = f\"{models_dir}/resnet_trained_for_cifar10.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_inputs,\n",
    "        onnx_model_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_onnx_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this process, observe the following output:\n",
    "\n",
    "* The trained ResNet-50 model on the CIFAR-10 dataset is saved at the following location in ONNX format: `onnx/resnet_trained_for_cifar10.onnx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the ONNX model\n",
    "\n",
    "Generated and adapted using Netron\n",
    ">Netron is a viewer for neural network, deep learning and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "\n",
    "<strong>Note</strong> this is an image of the default model we are using. If you have modified or re-trained your model, please visit [Netron](https://netron.app/) to generate a graph for your model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/png/short_onnx_unquantized_model.png\" alt=\"onnx_graph\" style=\"max-height: 700px; width:auto; height:auto;\">\n",
    "</center>\n",
    "<center><strong>ResNet-50 ONNX model</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Quantize the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing AI models from floating-point to 8-bit integers reduces computational power and the memory footprint required for inference. For model quantization, you can either use [Vitis AI quantizer](https://ryzenai.docs.amd.com/en/latest/vai_quant/vai_q_onnx.html) or [Microsoft Olive](https://ryzenai.docs.amd.com/en/latest/olive_quant.html). This example utilizes the Vitis AI ONNX quantizer workflow. \n",
    "   \n",
    "This will generate a quantized model using QDQ quant format and UInt8 activation type and Int8 weight type. After the run is completed, the quantized ONNX model `resnet.qdq.U8S8.onnx` is saved to `onnx/resnet.qdq.U8S8.onnx`.\n",
    "    \n",
    "For more information on representation of quantized ONNX models (e.g., QDQ quant format, UInt8 activation type and Int8 weight type) see [here](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#onnx-quantization-representation-format)   \n",
    "   \n",
    "The  ```quantize_static``` function applies static quantization to the model. \n",
    "\n",
    "```python\n",
    "from onnxruntime.quantization import QuantFormat, QuantType\n",
    "import vai_q_onnx\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    dr,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    enable_dpu=True, \n",
    "    extra_options={'ActivationSymmetric': True} \n",
    ")\n",
    "```\n",
    "\n",
    "The parameters of this function are:\n",
    "\n",
    "* **input_model_path**: (String) The file path of the model to be quantized.\n",
    "* **output_model_path**: (String) The file path where the quantized model will be saved.\n",
    "* **dr**: (Object or None) Calibration data reader that enumerates the calibration data and producing inputs for the original model. In this example, CIFAR10 dataset is used for calibration during the quantization process.\n",
    "* **quant_format**: (String) Specifies the quantization format of the model. In this example we have used the QDQ quant format.\n",
    "* **calibrate_method**:(String) In this example this parameter is set to ``vai_q_onnx.PowerOfTwoMethod.MinMSE`` to apply power-of-2 scale quantization. \n",
    "* **activation_type**: (String) Data type of activation tensors after quantization. In this example, it is set to QUInt8 (Quantized Unsigned Int 8).\n",
    "* **weight_type**: (String) Data type of weight tensors after quantization. In this example, it is set to QInt8 (Quantized Int 8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to define the calibration data reader (`resnet_calibration_reader`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "class CIFAR10DataSet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = data_dir\n",
    "        self.vld_path = data_dir\n",
    "        self.setup(\"fit\")\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]\n",
    "        )\n",
    "        self.train_dataset = CIFAR10(root=self.train_path, train=True, transform=transform, download=False)\n",
    "        self.val_dataset = CIFAR10(root=self.vld_path, train=True, transform=transform, download=False)\n",
    "\n",
    "\n",
    "class PytorchResNetDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        input_data = sample[0]\n",
    "        label = sample[1]\n",
    "        return input_data, label\n",
    "\n",
    "\n",
    "def create_dataloader(data_dir, batch_size):\n",
    "    cifar10_dataset = CIFAR10DataSet(data_dir)\n",
    "    _, val_set = torch.utils.data.random_split(cifar10_dataset.val_dataset, [49000, 1000])\n",
    "    benchmark_dataloader = DataLoader(PytorchResNetDataset(val_set), batch_size=batch_size, drop_last=True)\n",
    "    return benchmark_dataloader\n",
    "\n",
    "\n",
    "class ResnetCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, data_dir: str, batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.iterator = iter(create_dataloader(data_dir, batch_size))\n",
    "\n",
    "    def get_next(self) -> dict:\n",
    "        try:\n",
    "            images, labels = next(self.iterator)\n",
    "            return {\"input\": images.numpy()}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def resnet_calibration_reader(data_dir, batch_size=16):\n",
    "    return ResnetCalibrationDataReader(data_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to quantize and save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "Calibrated and quantized model saved at: onnx/resnet.qdq.U8S8.onnx\n"
     ]
    }
   ],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "# `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "input_model_path = \"onnx/resnet_trained_for_cifar10.onnx\"\n",
    "\n",
    "# `output_model_path` is the path where the quantized model will be saved.\n",
    "output_model_path = \"onnx/resnet.qdq.U8S8.onnx\"\n",
    "\n",
    "# `calibration_dataset_path` is the path to the dataset used for calibration during quantization.\n",
    "calibration_dataset_path = \"onnx/data/\"\n",
    "\n",
    "# `dr` (Data Reader) is an instance of ResNet50DataReader, which is a utility class that \n",
    "# reads the calibration dataset and prepares it for the quantization process.\n",
    "dr = resnet_calibration_reader(calibration_dataset_path)\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    dr,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    enable_dpu=True,\n",
    "    extra_options={'ActivationSymmetric': True} \n",
    ")\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the quantization process, observe the following output:\n",
    "\n",
    "* The quantized ResNet-50 model on the CIFAR-10 dataset is saved at the following location in ONNX format: `onnx/resnet.qdq.U8S8.onnx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Deploy the Model on NPU for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "\n",
    "To run Inference using the model generated in this notebook please refer to the [PyTorch ONNX Inference](5_1_pytorch_onnx_inference.ipynb) notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License 1\n",
    "\n",
    "```python\n",
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "# --------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License 2\n",
    "\n",
    "```python\n",
    "#################################################################################  \n",
    "# License\n",
    "# Ryzen AI is licensed under `MIT License <https://github.com/amd/ryzen-ai-documentation/blob/main/License>`_ . Refer to the `LICENSE File <https://github.com/amd/ryzen-ai-documentation/blob/main/License>`_ for the full license text and copyright notice.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "Copyright&copy; 2023 AMD, Inc\n",
    "</center>\n",
    "<center>\n",
    "SPDX-License-Identifier: MIT\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

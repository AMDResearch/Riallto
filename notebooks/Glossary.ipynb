{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7721cd1-e866-4abc-99cb-e3835025e557",
   "metadata": {},
   "source": [
    "# Glossary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394dd54",
   "metadata": {},
   "source": [
    "## APU \n",
    "\n",
    "Application Processing Unit - a compute chip that combines a x86 CPU, GPU and NPU integrated in the same chip. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ceac6-4d9b-48b1-8508-a8a9d393a807",
   "metadata": {},
   "source": [
    "## access pattern (in relation to data in memory)\n",
    "\n",
    "How data is read or written in memory. E.g., data can be accessed sequentially, randomly, or other ordered patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e83b1-4096-4f1b-9667-03d3ea59d990",
   "metadata": {},
   "source": [
    "## application\n",
    "\n",
    "Refers to the complete Ryzen AI NPU application. It consists of the configuration for the parts of the NPU that are used for this application, software kernel executable code for each AI Engine in a compute tile, connections between tiles, instructions for data movement in the NPU. The application is contained in a .xclbin file which is a container file that includes the other configuration and executable files for components in the NPU.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f77356-7d4e-42a4-9c06-3dd81060e152",
   "metadata": {},
   "source": [
    "## builder\n",
    "\n",
    "Riallto Python module used to build an NPU application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ddcf3a-0928-4518-a497-e44856e74a21",
   "metadata": {},
   "source": [
    "## callgraph\n",
    "\n",
    "Riallto Python object that is used to defines the connections and the data movement between tiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a92763-c180-4040-bed4-8976d0afa9b0",
   "metadata": {},
   "source": [
    "## compute tile\n",
    "\n",
    "One of the three tile types that make up the NPU. Contains the AI Engine processor and local data memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca197a-aab4-4afd-af54-537ea4ee9946",
   "metadata": {},
   "source": [
    "## dataflow graph\n",
    "\n",
    "Abstract representation of an application that defines the flow of data between compute nodes. Can be represented graphically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdc5ef-22e8-4fed-b46c-55ce8db0bb64",
   "metadata": {},
   "source": [
    "## data movers \n",
    "\n",
    "Control logic that moves data between tiles over point to point connections. Each tile will have a component of the data mover to send or receive data. In the sending tile, the data mover master initiates the data movement. On the other end of the connection in the receiving tile, a corresponding data move slave receives the data and stores it in memory or moves it to the processor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cd637-dd5b-414d-9f2a-9f27f1812ecd",
   "metadata": {},
   "source": [
    "## graph\n",
    "\n",
    "The graph describes the connections between tiles in the Ryzen AI NPU and the data movement between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15c826-8dde-4829-85c3-f86a199b7fc9",
   "metadata": {},
   "source": [
    "## IPU (Inference Processing Unit)\n",
    "\n",
    "Alternative name for an NPU. Special compute unit optimized for machine learning computation. Ryzen AI is the AMD brand name and is an instance of an NPU/IPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b931e-3e12-4d9e-9339-5603ae29d98c",
   "metadata": {},
   "source": [
    "## interface tile\n",
    "\n",
    "One of the three tile types that make up the NPU. Contains the data movers to move data from external system memory to tiles in the NPU, and receives data from the NPU and sends it to external system memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4906dd-2651-4715-8435-53ab0aae25f8",
   "metadata": {},
   "source": [
    "## kernel (or software kernel)\n",
    "\n",
    "The software function that runs on an AI Engine in a Compute tile. A software kernel has source code that is compiled to an executable that will run on an AI Engine. One kernel is usually assigned to one AI Engine. More than one kernel can be assigned to, and run on a single AI Engine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d5db5-753c-4f87-baac-87d8d7926c1e",
   "metadata": {},
   "source": [
    "## memory tile\n",
    "\n",
    "One of the three tile types that make up the NPU. Contains the data mover and local data memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddceca-7dee-4795-98ab-3f3825e3b805",
   "metadata": {},
   "source": [
    "## AIEtools\n",
    "\n",
    "The compilation tools used to build the Ryzen AI NPU application and are installed as part of Riallto.\n",
    "    \n",
    "## MLIR\n",
    "\n",
    "MLIR, or Multi-Level Intermediate Representation, is a compiler infrastructure project. It provides a common intermediate representation that allows for seamless communication and transformation across different programming languages and hardware targets. Riallto compiles software kernels written in Python and C++ into MLIR which is then used by the AIEtools to build Ryzen AI applications.  \n",
    "\n",
    "See: https://mlir.llvm.org/\n",
    "\n",
    "## MLIR-AIE\n",
    "\n",
    "[MLIR-AIE](https://xilinx.github.io/mlir-aie/) is an open-source research project from the Research and Advanced Development group (RAD) at AMD. MLIR-AIE is an intermediate tool used by Riallto to compile software kernels for the AI Engines in compute tiles. It will take automatically generated MLIR and compile this into executables for the AI Engines.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f42fff-659a-4cbe-88b4-30896c2274a7",
   "metadata": {},
   "source": [
    "## multidimensional\n",
    "\n",
    "This refers to memory access patterns and for the Ryzen AI, multidimensional 2D, 3D and 4D transfers are supported by memory tile data movers. Unlike linear data structures (like arrays), multidimensional data can be organized in tables, matrices, or hypercubes. Moving this type of data involves coordinating across multiple axes or indices, enabling operations that consider relationships in multiple dimensions simultaneously. This is common in machine learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15c826-8dde-4829-85c3-f86a199b7fc9",
   "metadata": {},
   "source": [
    "## NPU (Neural Processing Unit)\n",
    "\n",
    "Special compute unit optimized for machine learning computation. Also known as an Inference Processing Unit (IPU). Ryzen AI is the AMD brand name and is an instance of an NPU/IPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16196829-ca9e-4926-8afd-98c224f01a86",
   "metadata": {},
   "source": [
    "## numpy\n",
    "\n",
    "NumPy is a powerful numerical computing library for Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of high-level mathematical functions to operate on these arrays. NumPy is widely used in scientific and data-related tasks for its efficiency and ease of use, allowing for fast and convenient manipulation of numerical data in Python programs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b209bfd-e153-437e-9ef1-dd0da06cde33",
   "metadata": {},
   "source": [
    "## partition\n",
    "\n",
    "In algorithms or parallel computing, partitioning refers to dividing a set of data into subsets based on specific criteria. This can be done for efficient processing, distribution across multiple processors, or organizing data in a way that suits a particular algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ee915",
   "metadata": {},
   "source": [
    "## Ryzen AI\n",
    "\n",
    "Ryzen AI is the AMD brand name for the NPU (Inference Processing unit) in the Ryzen 7040 'Phoenix' computer chips. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a3c99-e177-48dc-8a09-15db2468072d",
   "metadata": {},
   "source": [
    "## schedule\n",
    "\n",
    "In a data flow architecture, a schedule refers to the plan or order in which operations or tasks are executed based on the availability of data dependencies. It determines the timing and sequence of operations in a computational graph, ensuring that each operation receives the required input data before it is executed. Scheduling is crucial for optimizing parallelism and minimizing idle time in a system, ultimately improving the overall efficiency of data flow computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ee87e-64ab-43ad-8360-d6bf58673f39",
   "metadata": {},
   "source": [
    "## shape (in relation to data)\n",
    "\n",
    "The shape of data refers to the structure and dimensions of a dataset. It describes how the data is organized in terms of rows, columns, and, more generally, its size along each dimension. For example, in a two-dimensional array, the shape might be expressed as (rows, columns). Understanding the shape of data is essential for proper manipulation, analysis, and processing, as it defines the layout and organization of information within the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f628ae-351e-47dd-be99-1e00b2d2f4af",
   "metadata": {},
   "source": [
    "## slice\n",
    "\n",
    "A slice refers to a subset or portion of a data structure. This concept is commonly used in arrays, lists, or other sequence-like data types. Slicing allows you to extract a specific range or segment of elements from the original data structure. In languages like Python, you can use slicing notation to specify the start and end indices to create a slice of a list or array. Slices are useful for working with parts of data without modifying the original structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c895a602-63fe-4e93-a954-0f623f88bee9",
   "metadata": {},
   "source": [
    "## template\n",
    "\n",
    "In programming, a template generally refers to a parameterized structure that can be customized for various specific instances. Templates are commonly used in programming languages to create generic classes or functions that can work with different data types without sacrificing type safety.\n",
    "\n",
    "In Riallto templates are provided for commonly used image processing data movement patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90e610-975a-4340-8388-67c347b53ae6",
   "metadata": {},
   "source": [
    "## tile\n",
    "\n",
    "The Ryzen AI NPU is structure as an array of compute, memory and interface tiles. Tile can refer to any or all of these three types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397ef7c-5890-46da-af57-2c0e053bc628",
   "metadata": {},
   "source": [
    "## tiling (not to be confused with tile)\n",
    "\n",
    "Tiling refers to data access patterns, and describes how data is broken down into smaller chunks for more efficient processing. It optimizes memory use and enhances parallelism by working on smaller subsets, improving overall computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521958c9-db5a-453f-af2e-187c5876e5c9",
   "metadata": {},
   "source": [
    "## vector\n",
    "\n",
    "A vector typically refers to a one-dimensional array or list of elements. Vectors are used to represent and manipulate sequences of data, such as numbers, characters, or other types. In a mathematical context, vectors often denote both magnitude and direction, but in programming, a vector is a simple and versatile data structure for storing and processing ordered collections of elements.\n",
    "\n",
    "The NPU AI Engines in compute tiles are vector processors and can process vectors (a series of elements) in parallel. For example, if you have two vectors, each of say 16 elements, a vector addition would add each element in the two vectors together in parallel. Other vector operations are supported by the AI Engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b2150-ad9b-4bf0-8ab0-0fb83eaa29aa",
   "metadata": {},
   "source": [
    "## vectorization factor\n",
    "\n",
    "The vectorization factor represents the number of elements processed simultaneously in a single vector instruction. It quantifies the level of parallelism achieved when using vector instructions to operate on data. For example, if the vectorization factor is 4, it means that four elements are processed at once, improving computational efficiency by performing multiple operations in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e26b4",
   "metadata": {},
   "source": [
    "## VPU (Vector Processing Unit)\n",
    "\n",
    "floating-point and fixed-point datapaths for SIMD (vectorial) computation on the AI Engine processor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40d459-cbd0-4ee1-b939-91f831448577",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center>\n",
    "Copyright&copy; 2023 AMD, Inc\n",
    "</center>\n",
    "<center>\n",
    "SPDX-License-Identifier: MIT\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
